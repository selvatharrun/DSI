{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SVNWqs3Y6ud2domYRcBJgffPY56EVXsL",
      "authorship_tag": "ABX9TyNaECBAjJzYEBpnPncDSRsr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/selvatharrun/DSI/blob/main/VGG16_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: augment this dataset for vgg16\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/deep learning /catsAndDogs40/train\"\n",
        "test_path = \"/content/drive/MyDrive/deep learning /catsAndDogs40/test\"\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    preprocessing_function=preprocess_input  # Important for VGG16\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preprocess_input)  # Only rescale for testing\n",
        "\n",
        "# Generate augmented training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(224, 224),  # Input size for VGG16\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # Assuming binary classification (cats vs. dogs)\n",
        ")\n",
        "\n",
        "# Generate testing data (no augmentation)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Now you can use train_generator and test_generator to train your VGG16 model.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GGyavqo1n8R",
        "outputId": "79f70fba-f1a8-40eb-fec0-237b7877a792"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 64 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained VGG16 model (excluding the top classification layers)\n",
        "base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers in the base model (optional, but often helpful for transfer learning)\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Add your own classification layers on top of the VGG16 base\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification (cats vs. dogs)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrN5aKtI3dpB",
        "outputId": "240ee8bb-dfdf-4399-e22f-5712aefc4f2b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(train_generator,validation_data = test_generator, epochs=15)"
      ],
      "metadata": {
        "id": "zIoMUw2J2TBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d49b11e7-716f-4e42-86d9-9245e83b669a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 38s/step - accuracy: 0.5521 - loss: 1.8756 - val_accuracy: 0.5000 - val_loss: 2.7281\n",
            "Epoch 2/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 31s/step - accuracy: 0.5938 - loss: 1.5441 - val_accuracy: 0.6250 - val_loss: 0.3791\n",
            "Epoch 3/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 30s/step - accuracy: 0.7917 - loss: 0.5374 - val_accuracy: 0.7500 - val_loss: 0.7408\n",
            "Epoch 4/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 31s/step - accuracy: 0.7396 - loss: 0.9288 - val_accuracy: 0.8750 - val_loss: 0.3935\n",
            "Epoch 5/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 30s/step - accuracy: 0.8229 - loss: 0.4268 - val_accuracy: 0.8750 - val_loss: 0.4580\n",
            "Epoch 6/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 30s/step - accuracy: 0.8542 - loss: 0.3883 - val_accuracy: 0.9375 - val_loss: 0.3099\n",
            "Epoch 7/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 30s/step - accuracy: 0.7917 - loss: 0.4190 - val_accuracy: 0.8750 - val_loss: 0.2978\n",
            "Epoch 8/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 31s/step - accuracy: 0.9062 - loss: 0.2218 - val_accuracy: 0.9375 - val_loss: 0.2881\n",
            "Epoch 9/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 30s/step - accuracy: 0.9792 - loss: 0.0735 - val_accuracy: 0.9375 - val_loss: 0.3356\n",
            "Epoch 10/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 40s/step - accuracy: 0.9583 - loss: 0.1408 - val_accuracy: 0.8750 - val_loss: 0.4877\n",
            "Epoch 11/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 34s/step - accuracy: 0.9479 - loss: 0.1481 - val_accuracy: 0.8750 - val_loss: 0.4905\n",
            "Epoch 12/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 30s/step - accuracy: 0.9792 - loss: 0.1214 - val_accuracy: 0.8750 - val_loss: 0.4059\n",
            "Epoch 13/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 30s/step - accuracy: 0.9688 - loss: 0.1381 - val_accuracy: 0.9375 - val_loss: 0.2854\n",
            "Epoch 14/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 30s/step - accuracy: 0.9792 - loss: 0.0686 - val_accuracy: 0.8750 - val_loss: 0.2721\n",
            "Epoch 15/15\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 30s/step - accuracy: 0.9062 - loss: 0.1716 - val_accuracy: 0.8750 - val_loss: 0.2757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_img = \"/content/drive/MyDrive/deep learning /catsAndDogs40/test/cat/1.jpg\"\n",
        "import numpy as np\n",
        "\n",
        "img = tf.keras.utils.load_img(t_img, target_size=(224, 224))\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = np\n",
        "\n"
      ],
      "metadata": {
        "id": "Sn6-MJ1_3vvI"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}